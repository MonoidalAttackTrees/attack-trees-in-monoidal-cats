Before introducing ATLL and Indexed ATLL and how it can be used to
model and reason about attack trees I first give a brief introduction
to (ordered) linear logic and how it can be used to model various
structures in computer science.  I assume very little of the reader,
and start with the basics of the specification of logics.  Ordered
linear logic (OLL) \cite{Polakow:2001} will be the ongoing example
throughout this section, because it is the logic ATLL is based on.

Every logic presented in this paper is in the form of sequent-style
natural deduction.  This type of formalization begins with the
specifying the syntax of formulas and sequents.  The latter is defined
by a set of inference rules.  The syntax for OLL is as follows:
\[
\begin{array}{crcl}
\text{(formulas)} & [[A]],[[B]],[[C]] & ::= & [[b]] \mid [[A (.) B]]
  \mid [[A > B]] \mid [[A -o B]]\\
\text{(ordered contexts)} & [[D]] & ::= & [[.]] \mid [[A]] \mid [[G,D]]\\
\text{(unordered contexts)} & [[G]] & ::= & [[.]] \mid [[A]] \mid [[I,P]]\\
\end{array}
\]
Formulas of OLL consist of atomic formulas denoted by $[[b]]$, a
symmetric tensor product denoted by $[[A (.) B]]$, a non-symmetric
tensor product denoted by $[[A > B]]$ -- in ATLL the former will be
used to model sequential conjunction, and the latter to model parallel
conjunction -- and linear implication denoted by $[[A -o B]]$.
Contexts are lists of hypothesis where $[[D]]$ is a list of ordered
hypothesis, and $[[G]]$ is a list of unordered hypothesis.  The former
are associated with the non-symmetric tensor product, and the latter
with the symmetric tensor product.  We denote the empty context by
$[[.]]$, and appending of contexts $[[G1]]$ and $[[G2]]$ by
$[[G1,G2]]$.

Sequents of OLL are denoted by $[[D;G |- A]]$.  We say that the
sequent $[[D;G |- A]]$ holds if and only if given the hypothesis in
$[[D]]$ and $[[G]]$ one can construct a proof of the formula $[[A]]$
using the following inference rules:
\begin{mathpar}
  \OLLdrulevar{} \and
  \OLLdrulevarC{} \and
  \OLLdruleparaI{} \and
  \OLLdruleparaE{} \and
  \OLLdruleseqI{} \and
  \OLLdruleseqE{} \and
  \OLLdruleex{} \and
  \OLLdruleimpI{} \and
  \OLLdruleimpE{}
\end{mathpar}
An inference rule should be read from top to bottom as an implication.
The sequents on top of the line are the premises, and the sequent
below the line is the conclusion.  For example, one should read the
rule $\OLLdruleparaIName{}$ as if $[[D1;G1 |- A]]$ and $[[D2;G2 |-
    B]]$ both hold, then $[[D1,D2;G1,G2 |- A (.) B]]$ holds.  An
inference rule with no premises are called axioms.  For example, the
rules $\OLLdrulevarName{}$ and $\OLLdrulevarCName{}$ are both axioms.

Inference rules are read from top to bottom, but applied from bottom
to top.  A sequent, $[[D;G |- A]]$, holds if and only if there is a
series of inference rules that can be composed into a derivation.
Every derivation is a tree where the root is the sequent we wish to
prove, and then rules are composed to form a tree.  A rule can be
composed with another if the formers conclusion matches a premise of
the latter.  If every branch of this tree reaches an axiom, then it is
a valid proof, but if any branch gets stuck, then the sequent does not
hold. The following is a proof that the symmetric tensor is indeed
symmetric:
\begin{center}
  \begin{math}
    \inferrule* [right=$\OLLdruleimpIName{}$] {
      $$\mprset{flushleft}
      \inferrule* [right=$\OLLdruleparaEName{}$] {
        $$\mprset{flushleft}
        \inferrule* [right=$\OLLdrulevarName{}$] {
          \,
        }{[[. ; A (.) B |- A (.) B]]}
        \\
        $$\mprset{flushleft}
        \inferrule* [right=$\OLLdruleexName{}$] {
          $$\mprset{flushleft}
          \inferrule* [right=$\OLLdruleparaIName{}$] {
            $$\mprset{flushleft}
            \inferrule* [right=$\OLLdrulevarName{}$] {
              \,
            }{[[. ; B |- B]]}
            \\
            $$\mprset{flushleft}
            \inferrule* [right=$\OLLdrulevarName{}$] {
              \,
            }{[[. ; A |- A]]}
          }{[[. ; B, A |- B (.) A]]}
        }{[[. ; A, B |- B (.) A]]}
      }{[[. ; A (.) B |- B (.) A]]}
    }{[[. ; . |- (A (.) B) -o (B (.) A)]]}
  \end{math}
\end{center}
We call the root of the tree the goal of the proof, and as we
construct a derivation this goal is refined into potentially several
new subgoals.  This is called goal directed proof.  As we can see the
previous derivation is a valid proof of the goal $[[. ; . |- (A (.) B)
    -o (B (.) A)]]$.

The following derivation is an invalid proof that the non-symmetric
tensor product is symmetric:
\begin{center}
  \begin{math}    
    \inferrule* [right=$\OLLdruleimpIName{}$] {
      $$\mprset{flushleft}
      \inferrule* [right=$\OLLdruleseqEName{}$] {
        $$\mprset{flushleft}
        \inferrule* [right=$\OLLdrulevarName{}$] {
          \,
        }{[[. ; A > B |- A > B]]}
        \\
          [[A, B ; . |- B > A]]
      }{[[. ; A > B |- B > A]]}
    }{[[. ; . |- (A > B) -o (B > A)]]}
  \end{math}
\end{center}
At this point we have refined our goal to the subgoal $[[A, B ; . |- B
    > A]]$, but this is impossible to prove, because the exchange rule
$\OLLdruleexName{}$ cannot be be applied to the ordered context to
commute $[[A]]$ and $[[B]]$.

Building proofs from the inference rules given above can often be
tedious and long.  To make this process easier it is often necessary
to introduce new rules that can be proven to be valid in terms of the
inference rules already given.  Rules introduced in this way are
called derivable inference rules.  One derivable rule we will make use
of is the following:
\[
\OLLdrulecomp{}
\]
Proving this rule is derivable amounts to creating a derivation whose
branches terminate at either an axiom or the premises
$[[D2;G2 |- A -o B]]$ or $[[D1;G1 |- B -o C]]$.  The proof is as follows:
\[
\inferrule* [right=$\OLLdruleimpIName{}$] {
  $$\mprset{flushleft}
  \inferrule* [right=$\OLLdruleimpEName{}$] {
    [[D1;G1 |- B -o C]]
    \\
    $$\mprset{flushleft}
    \inferrule* [right=$\OLLdruleimpEName{}$] {
      [[D2;G2 |- A -o B]]
      \\
        $$\mprset{flushleft}
      \inferrule* [right=$\OLLdrulevarName{}$] {
        \,
      }{[[.;A |- A]]}
    }{[[D2;G2,A |- B]]}          
  }{[[D1,D2;G1,G2, A |- C]]}
}{[[D1,D2;G1,G2 |- A -o C]]}
\]

At this point I have introduced the basics of sequent-style natural
deduction, but nothing I have said up to now has been specific to
ordered linear logic.  We call the this logic ordered, because it
contains the non-symmetric tensor product.  Its non-symmetry is
enforced by the separation of hypotheses.  What makes a logic linear?

Linear logic was first introduced by Girard~\cite{Girard:1987} in the
late eighties.  A logic is linear if every hypothesis is used exactly
once.  Thus, no hypothesis can be duplicated or removed at will, that
is, the structural rules for contraction (duplication) and weakening
(removal) must be banned from the logic.  This property makes linear
logic particularly suited for reasoning about state-based systems.

We enforce this property by restricting the inference rules in two
ways. First, consider the identity axioms for OLL:
\[
\begin{array}{lll}
  \OLLdrulevar{} & \quad\quad\quad\quad & \OLLdrulevarC{}
\end{array}
\]
The contexts are restricted to containing at most the hypothesis the
axiom is proving holds.  The other restriction is when applying the
inference rules during the construction of derivations one is not
allowed to copy or introduce new hypothesis across premises.  This is
why we take great care at separating the contexts in the definition of
the inference rules.

Modeling structures in logic corresponds to interpreting the
structures as formulas, and then reasoning about the structures
corresponds to proving implications between the interpretations.  As
we will see, attack trees will be modeled as formulas of linear logic,
and then we will prove several properties of attack trees by proving
implications between them, but before moving on to ATLL I give an
example showing how to model a simple state-based system in linear
logic.

There is a common example of problem solving using in artificial
intelligence called the block world scenario; I learned of this
example from Power and Webster's work on embedding linear logic in Coq
\cite{?}.  Suppose there is a finite set of blocks and a robot arm
that be used to move blocks around.  For example, the arm might break
a single stack of blocks into two stacks.  This setup can be modeled
in linear logic using a few predicates on blocks.

The state of the system can be modeled using the following predicates:
\begin{center}
  \begin{tabular}{|rll|}
    \hline
    $[[on x y]]$  & : & block $x$ is on top of block $y$\\
    $[[table x]]$ & : & block $x$ is on the table (i.e. no block underneath of $x$)\\
    $[[clear x]]$ & : & there is no block on top of $x$\\
    $[[holds x]]$ & : & the robot arm is holding block $x$\\
    $[[empty]]$  & : & an atomic formula indicating the robot arm is not holding a block.\\
    \hline
  \end{tabular}
\end{center}
Next we must characterize the valid actions one can perform during the
game.  These are defined as a set of linear logic axioms.  The
following set of inference rules axiomatize the actions:
\begin{mdframed}
  \begin{mathpar}
    1
  \end{mathpar}
\end{mdframed}
